---
title: Performance Metrics
tags:
draft: "false"
---
# Evaluation 
We have several ways of deal with error and seeing how good a matrix is. We can use a [[Confusion Matrix]] to calculate [[True Positive]]s, [[True Negative]]s, [[False Positive]]s, and [[False Negative]]s. We can also use [[Accuracy]]/[[Misclassifcation Error]], [[Precision]], [[Recall]], [[F-1 Score]] for data with not many values, and [[ROC]] curves as well as [[AUROC]].