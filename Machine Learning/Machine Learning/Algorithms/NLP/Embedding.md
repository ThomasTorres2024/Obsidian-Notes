---
title: Embedding
tags:
draft: "false"
---
# Embedding 
take data of 1 form, then inbed it ia new spae, in ml we do thsis enerally to handle all of the data 

for instance if we want t embed content from say an image, 10x10x3 into a vector field, and then 
embed which we can generate fom nis intermdiate stat

Good embeddings have simila rej=-qualiities have, simialr things, can bring them to class space 

foundation model, trained ona  large dataset, USEFULIN COMPLEX ESEMANTIC AREAS, for 1 giant set, we can use a f c==nction to encapsulate an entir region 

machine transls, how could we use embeddij

Let $V$ be a 50,000 dikensional vector, and let $F$ lso ben, we we can encode as a 


transform all sets of embeddings to a larger a =sifodl. If the map of  2 sentences with the same meaning map really well, then we can do  SOMETHING

WE CAN ALSO unencode encoded data and also we encoden them, information sometimes is lost embedding/encoding/feature transforms are simialr 

We can use kernel transformaon o signficiatny to help us and our data, especially data tha ras our daa may ive on a qadratic, but we are able to make thes with linear transformation because we used a feature transformation on them firsr 

spiral set, if we transform our spiral set, and use a kernel, using different colors and stuff we get 
a new set 