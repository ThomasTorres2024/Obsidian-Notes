---
title: Tokenization
tags:
draft: "False"
---
# Tokenization 
[[Tokenization]] is the process of converting a sequence of words in a sentence into an [[Embedding]] in some high dimensional space. 