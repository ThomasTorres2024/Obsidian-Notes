---
title: Eigenfaces
tags:
draft: "False"
---
# Background
Eigenfaces leverage [[Principal Component Analysis (PCA)]] in order to identify a face from a given dataset of faces in a computationally efficient manner. Eigenfaces can add new members to their overall dataset, and identify new faces added to the set.

Previous algorithms existed which already were somewhat successful at facial recognition, though these focused primarily on recognizing features of faces like eyes, nose, mouth, head outline, etc, and then trying to find relationship between these elements, however this particular model was fragile.

Many models before Eigenfaces ignored the key problem of asking what aspects of the face are actually important for identification? 
The eigenface approach tries to go about this through an "Information Theory Approach", where we want to extract relevant information in a face, encode it, and compare different face encodings.

This essentially means applying PCA to find the principal components and thereby most important characteristics in the face. Other approaches looked at specific features that would be important to humans for identification, like nose, face, ear structure but not necessarily other structure. 

---
# Algorithm Overview

We begin by taking the dataset of faces, $D$, and breaks the dataset into a smaller set of characteristic images. These smaller characteristic images are eigenfaces, and are thought of as the [[Principal Component]]s of the training set of face images. 

A new image is classified by projecting it down into the "face space", and classifying the face by comparing its position with the face of already known persons. This is am important feature, especially if we want to recognize people that we already have seen within our dataset. 

The training set constrains itself to a limited set of angles of an individual, a $45^\degree$ angle, a profile view, and a straight on view, which overall allows the algorithm to classify faces from many different angles.  

Each image in our dataset is assumed square, black-white, and of size $n \times n$. We can represent each image as a vector residing in $\mathbb{R}^{n^2}$. Note that this a very high dimensional space even if the image is relatively small. A $16 \times 16$ image resides in $\mathbb{R}^{256}$. Let each image in the dataset be represented by $\vec{x_{i}}$. 

Given $D=\{\vec{x_{1}},\vec{x_{2}},\cdots,\vec{x_{m}} \}$ where each $\vec{x_{i}} \in \mathbb{R}^{n^2}$. We begin by performing PCA, which means determining the [[Covariance Matrix]] of $D$, and then subtracting the mean vector, $\vec{\mu}= \frac{1}{m} \sum_{i}^m \vec{x_{i}}$.  

The covariance matrix of $D$,  is a [[Symmetric Matrix]] and is thereby [[unitarily diagonalizable]], and so it has a non-unique factorization as:
$$C=X\Lambda X^{T}$$
In the eigenface algorithm, we particularly choose to order $\Lambda$ by descending eigen values such that the highest principal component is first, and we organize each eigen vector as such. We can think of the [[eigen vectors]] as some feature between the images which has its own respective weight of its corresponding eigen value. We can actually view some of the images generated by this procedure, since the generated eigen vector is another 256 dimensional entry.  Eigenfaces have a very ghostly appearance to them. 

Because we have a full set of eigenvectors, the set of all images living in $\mathbb{R}^{n^2}$ can be represented as a linear combination of the eigen vectors from $X$. We can also perform a truncation to only consider the best eigen vectors, that is eigen vectors where for $\lambda > \epsilon_{threshold}$, we know that the corresponding eigen vector is more "important". This is similar to [[Truncated SVD]]. 

This particular idea for representing faces with [[Principal Component Analysis (PCA)]] comes from 2 other authors, Sirovich and Kirby, that made a method to efficiently represent face pictures, where essentially you could store some eigen pictures from a set of images (I'm pretty sure what they mean by this is much like the images here, we can get some eigen-vector representation of a face that spans the facespace, and then can find some appropriate linear combination such that any vector in the facespace is something we can represent). 

If we think about the size of these images and the amount of info we need to store them, any particular face is now storable as a few coefficients representing a sum over the eigenfaces.  

### Initialization Algorithm 
From this we can more formally describe the initialization of the facespace and eigenfaces. We run this process before our first training, or whenever we have spare time computationally. After we add in more faces, we want to add them to the existing set and train it. 

1. Get an initial set of face images, the training set 
2. Get the eigenfaces from the set with the highest corresponding [[eigen value]]s, these $M$ images span the "face space". As we encounter new faces, eigenfaces can be updated or recalculated. 
3. For every image in our Dataset, we can project it onto the facespace to determine the set of weights for each person to represent all (technically not all since $M$ is not necessarily all of the eigen-vectors) possible faces 

### Facial Recognition Step 
We can recognize new faces that are in our dataset by performing the following procedure. Note, it must be the case that our model has already been initialized. 
1. Take a new image, and calculate its weights by projecting the image onto each of the eigen faces 
2. Determine if the image is a face, which we do by checking to see if the image is close enough to the face space 
3. If the face can be determined to be a face, classify the weight pattern as a person that is known or unknown 
4. (Optional) Update Eigenfaces/Weight Patterns
5. (Optional) if we see the same unknown face multiple times, find its weight pattern and incorporate it into the known faces 

---
# Calculation of Eigenfaces 
To reiterate, if our images are $256 \times 256$, it turns out that our dataset lies in $R^{256^2}$, which is a massive space. However, faces are quite similar, and span a lower dimensional subspace in this much larger space. By using PCA, we hope to find the vectors which best define the face subspace within the larger image space (I'm pretty sure this is why we care about variable importance here)

I already wrote something similar to this part above but I'll do it again because we need to use their notation. Let the set of face images be $\Gamma_{1},\Gamma_{2},\cdots,\Gamma_{n}$. The average of the face set is given by:
$$\Psi=\frac{1}{M} \sum_{n=1}^M \Gamma_{n}$$
We denote the difference between the $i$th image and the mean as:
$$\Phi_{i}=\Gamma_{i}-\Psi$$
We then run PCA on the matrix of the new $\Phi_{i}$'s such that we obtain a set of $M$ orthonormal vectors, $u_{n}$ that describe the distribution of our data. We select our eigen vectors by considering the following formula for $k$:
$$\lambda_{k}=\frac{1}{M}\sum_{n=1}^M(u_{k}^T\Phi_{n})^2$$
$u_{k}$ is chosen such that $\lambda_{k}$ is a maximum subject to the [[Kronecker Delta]]:
$$u_{j}^Tu_{k}=\begin{cases} 1 & \text{if } i=j\\ 0 & \text{ else} \end{cases} $$
The vectors $u_{k}$ are the eigen vectors with the corresponding eigen values $\lambda_{k}$ of the [[Covariance Matrix]]:
$$C=\frac{1}{M} \sum_{n=1}^M \Phi_{n}\Phi_{n}^T=AA^T$$
Where $A=[\Phi_{1} \Phi_{2} \cdots \Phi_{M}]$. The resulting matrix, $C \in \mathbb{R}^{n^2 \times n^2}$, which requires us to find $n^2$ eigen vectors and eigen values, which is a tremendous amount of work.  

If $M < N^2$, it follows that there are only $M-1$ meaningful eigen vectors. All of the other eigen-vectors have an eigen value of 0, and are not very telling. 

It turns out that we can solve for the eigen vectors in an $M \times M$ matrix rather than the $N^2 \times N^2$ matrix, for instance solving a $16 \times 16$ matrix rather than solving the eigen values and vectors for $(128)^2 \times (128)^2$. 

There is a transformation we can do between eigen vectors of $A^TA$ and $AA^T$ which gets us an eigen vector of $AA^T$ for an additional cost. Using $A^TA$, we can write our an eigen vector-value for it:
$$A^TAv_{i}=\mu v_{i}$$
$$AA^TAv_{i}=\mu A v_{i}$$
$$AA^Ty=\mu y$$
Which means that $Av_{i}$ is an eigen vector of $AA^T$. All we need to do is compute $v_{i}$ and then we can basically get the eigen vectors of $AA^T$. We can now find the eigen values of $L=A^TA$, where $L_{mn}=A_{m}^TA_{n}$, from which can we can find the $M$ eigenvectors $v_{l}$ of $L$. These vectors can be used to compute the eigen vectors of $AA^T$, the covariance matrix:
$$u_{j}=Av_{j}=\sum_{i=1}^Mv_{ji}A_{i}$$
This brings the overall order down from a computation which is $O(n^2)$ to one which is $O(m)$. Generally, the number of training images is quite small compared to the size of these images, so:
$$M \ll N^2$$
Finding the eigen values here allows us to get an idea of how important each eigenface is. From this section, we have found a way to compute eigenfaces that span the facespace. We have found a way to do it in a manner which is computationally efficient and uses $M$, as well as connect it with the broader idea of PCA. 

### Face Classification 
The vectors $u_{j}$ that we just calculated span the facespace, and are moreover a basis for it since they are linearly independent because they are already known to be mutually orthonormal. We denote this facespace that the images span to be $M'$, which generally is smaller than the original $N^2$ image space. When choosing the eigen vectors to span $M'$, we choose the eigen vectors with the largest associated eigen values 

If we recall, since we have an orthogonal basis for our set, and a face image should belong to $M'$, we can calculate a representation of the new Image (since the image is a part of the face space assumable) with respect to our new coordinates by doing the following:
$$w_{k}=u^T_{k}(\Gamma-\Psi)=u_{k}^T\sum_{i=1}^Mc_{i}u_{i}^T$$
This makes sense because we can just annihilate the other orthogonal parts by left multiplying by each orthonormal vector we just generated. In turn, this gives us the corresponding weight needed at each step. So, we now have a vector of weights which describes the image, $\Gamma$, projected down onto the face subspace $M'$, let us denote this vector by:
$$\Omega^T=[w_{1},w_{2},\cdots w_{m}]$$We can interpret each weight in $\Omega$ as the amount that each eigenface contributes to representing our current image. We can then try and cross reference this weight vector with other weight vectors, and try and determine which of the vectors we already have within our seen/training set most closely matches our new vector $\Omega$. One simple way of doing this is by iterating over all such vectors in the train set and computing:
$$\text{kth error }=\epsilon_{k}=\|\Omega-\Omega_{k}\|^2$$
Each $\Omega_{k}$ is an average over some of the projections onto $M$ for an individual (I think this is because we can't really encode for 1 person just through 1 image, we need a couple images, maybe from different angles too which was something mentioned earlier). We can assign a face to a class when its error is beneath some chosen threshold, $\theta_{\epsilon}$ (one of the first hyperparameters I've seen reading this). If $e_{k} \not \leq \theta_{\epsilon}$, then we say that the face is unknown, and potentially we make a new face class for it. 

A point of concern about the layout of our problem is that when we project an image down onto $M'$, it is possible that the image looks nothing like a face, and is still close to actual faces within our face subspace. I'm a bit confused on this part, so I'll temporarily skip it. There's some way to calculate if you're close to a class? Not really sure.  

This is basically it for the baseline algorithm, and from this we have 4 outputs from classification
- 1.) We get a face we have already seen 
- 2.) We get a face, but we have not seen it so it is unknown (in face space but not near a known class)
- 3.) We get an image close to a face class, but not in the face space, it is not a face
- 4.) We get an image distant from a face class, and it is not in the face space, it is not a face 

---
# Base Algorithm Overview 

Here is the overall approach of the algorithm. 
1. Get a characteristic set of face images of known people, we need multiple face pictures from the same people, some variation in expression and lighting (for 10 people, 4 images each, so $M=40$)
2. Calculate $L$, and find its eigen values and then $M'$ with its eigenvectors such that we choose the most significant ones
3. Compute the eigenfaces, the eigen vectors of the Covariance matrix, $u_{k}$ 
4. For each known individual get the class vector, $\Omega_{k}$, average eigen face pattern vectors, choose a threshold for maximum dist from face class, and a threshold for the max distance from the face space
5. For any new image identified, calculate its pattern vector $\Omega$,  distances to each class $\epsilon_{i}$, distance to face space
	*  If $e_{k} < \theta_{e}$ and $\epsilon < \theta_{\epsilon}$, then classify the image as associated with individual $k$ 
	* If the minimum distance $\epsilon_{k} > \theta_{\epsilon}$ (above threshold for class), but distance puts us in the face space $(\epsilon \leq \theta_{\epsilon})$, then we have an unknown (possible to start a new face class here)
6. If the individual is seen already, then maybe we could add it to the original faces or recalculate eigenfaces, which makes sense as we encounter more and more individuals 