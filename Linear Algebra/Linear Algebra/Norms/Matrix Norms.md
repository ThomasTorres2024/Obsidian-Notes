---
title: Matrix Norms
tags: 
draft: "false"
---
# Matrix Norm Definition
Say for some matrix, $A \in \mathbb{R}^{m \times n}$, a matrix norm is a mapping from a vector space of matrices, $\mathcal{V}$ to $\mathbb{F}$.  We denote this norm function by:
$$\| \cdot \| : \mathbb{F}^{m \times n} \rightarrow \mathbb{R}$$All matrix norms must satisfy the following conditions:
1. $\| A \| \geq 0$ 
2. $\| A \| = 0 \Longleftrightarrow A = 0^{m \times n}$
3. $\| \alpha A \|=|a| \| A \|$ for $a \in \mathbb{F}$
4. $\| A +B \| \geq \| A\| + \| B \|$ The triangle equality for matrices.
5. $\|A \cdot B \| \leq \| A\| \cdot \| B \|$ Sub multiplicative Rule. (Note this is a generalization of the [[Cauchy-Schwarz Inequality]])

---
# Matrix $p$ Norms

#### 1.) $p$ Norms for a Matrix 
We can think of $p=2$ norm as being generated by the ratio of the norm $\|A\vec{x} \|$ and $\| \vec{x} \|$:
$$ \|A \|_{2} = \text{max} \left( \dfrac{\|A\vec{x} \|_{2}}{\| \vec{x} \|_{2}} \right)$$
Since our [[Vector Norms]] are well defined, we can simply extend the notion through the finding the maximum of the ratio over all values of $\vec{x}$. We suppose that this ratio is actually equal to $\sigma_{1}$, that is the largest singular value of the [[Single Value Decomposition]] of our matrix.. 

We can see that if $A$ were a symmetric matrix, that the  [[eigen vectors]] $\vec{x}$ would result in a ratio of $|\lambda|$. However, since $A$ is not guaranteed to be $n \times n$ or symmetric, we can instead use [[singular value]]. If we choose our first right [[singular vector]], our corresponding ratio will be the largest [[singular value]]. 

Note that: 
$$A=U\Sigma V^T$$
So, if $\vec{x}=\vec{v}_{i}$ our resulting products are:
$$A\vec{v}_{i}=U \Sigma V^T \vec{v}_{i}=U \Sigma \vec{e}_{1}=\sigma_{1}U\vec{e}_{1}=\vec{u}_{1}\cdot \sigma_{1}$$
If we now consider the ratio that we are trying to maximize, when $\vec{x}=\vec{v}_{i}$ we obtain:
$$ \dfrac{\|A\vec{v}_{1} \|_{2}}{\| \vec{v}_{1} \|_{2}} = \dfrac{\|  \sigma_{1}\cdot \vec{u}_{1} \|}{1}=\| \vec{u}_{1} \| \cdot |\sigma_{1}|=\sigma_{1}$$
We get the nice property for the vector norms of our singular vectors due to them being [[orthonormal]].  We can find more $p$ norms for matrices if we can consider other vector p-norms. 

#### 2.) Frobenius Norm
The Frobenius Norm comes from treating our matrix as a big vector, and then taking its $L2$ norm. Meaning, that we iterate over each row and column, and square each entry of the matrix, and then we have the new sum, we take its square root. This gives us our norm:
$$\| A \|_{F}=\sqrt{\sum_{i=1}^m \sum_{j=1}^n (A_{ij})^2}=\sqrt{\sum\text{sum of all } a_{ij}^2}$$
It turns out that this is actually also equivalent to square the  sum of the squares of the singular values of $A$:
$$\| A \|_{F}=\sqrt{\sum_{i=1}^n \sigma_{i}^2}=\sqrt{\sigma_{1}^2+\sigma_{2}^2+\cdots+\sigma_{n}^2}$$
We want to see why the Frobenius Norm connects to the definition given above. If we consider the SVD decomposition of $A$:
$$A=U\Sigma V^T$$
Taking the norm of $A$ given $U=V=I$ would be the equivalent of taking the norm of $\Sigma$ which would directly lend itself to the Frobenius singular value definition above, since we are merely summing over the sigmas. The intuition behind this formula is the fact that our $U$ is an [[orthogonal matrix]], and $V^T$ is an orthogonal matrix, and in turn have no bearing over the norm of  $A$.   

#### 3.) Nuclear Norm
We denote the nuclear norm of a matrix $A$ by $\| A\|_{N}$. This is also useful for deep learning. It turns out that we tend to have a lot more  weights than possibly samples. Our deep learning problem tends to have a lot of variables and minima. Our optimization via [[Gradient Descent]] is believed by professor Srebro at MIT to minimize the Nuclear Norm. 

All we do here is to sum the [[singular value]]s of our matrix:
$$\|A \|_{N}=\sum_{i=1}^n\sigma_{i}=\sigma_{1}+\sigma_{2}+\cdots+\sigma_{n}$$
---
# Norms Induced by Vectors
We can find [[Matrix Norms]] by using a vector as well. We can derive our notion of the size of a matrix from the size of vectors. The matrix norm is defined as the following between matrix norms on $p$ and $q$ on the domain and range of $A\in \mathbb{R}^{m \times n}$:
$$\|A\vec{x}\|_{p} \leq C\|\vec{x}\|_{q}$$
$C \in \mathbb{R}$ is the smallest number for which the above equality holds for any $\vec{x} \in \mathbb{R}^n$ where $\vec{x} \neq \vec{0}$. We think of this as the maximum factor by which $A$ is able to stretch the vector $\vec{x}$. We have that:
![[Pasted image 20250918103351.png]]

For instance, the induced vector norm of $1, \|A\|_{1}$ is given by:
$$\|A\|_{1}= \sup_{\vec{x} \in \mathbb{R}^n, \|\vec{x}\|_{1}=1} \|A\vec{x}\|_{1}$$
Which is equivalent to saying the maximum of 1 norm of column vectors of $A$. Which is equivalent to saying the maximum sum of the columns 